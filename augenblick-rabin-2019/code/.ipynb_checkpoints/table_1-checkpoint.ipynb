{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of Augenblick and Rabin, 2019, \"An Experiment on Time Preference and Misprediction in Unpleasant Tasks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors:  \n",
    "\n",
    "- Massimiliano Pozzi (Bocconi University, pozzi.massimiliano@studbocconi.it)\n",
    "- Salvatore Nunnari (Bocconi University, salvatore.nunnari@unibocconi.it)\n",
    "\n",
    "The code in this Jupyter notebook performs the aggregate estimates to replicate column 1 of Table 1\n",
    "\n",
    "This notebook was tested with the following packages versions:\n",
    "- Pozzi:   (Anaconda 4.10.3 on Windows 10 Pro) : python 3.8.3, numpy 1.18.5, pandas 1.0.5, scipy 1.5.0, numdifftools 0.9.40\n",
    "- Nunnari: (Anaconda 4.10.1 on macOS 10.15.7): python 3.8.10, numpy 1.20.2, pandas 1.2.4, scipy 1.6.2, numdifftools 0.9.39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from   scipy.stats import norm\n",
    "import scipy.optimize as opt\n",
    "import numdifftools as ndt\n",
    "from autograd import grad, hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning and Data Preparation\n",
    "\n",
    "We import the relevant dataset containing observations for all 100 individuals. We then construct the primary sample used for the aggregate estimates. This sample consists of 72 individuals whose individual estimates converged. To guarantee consistency with the authors' results we run their \"03MergeIndMLEAndConstructMainSample.do\" do file to create a csv file named \"ind_to_keep.csv\" containing the identifiers of the individuals to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the two datasets and drop subjects whose individual estimates do not converge\n",
    "\n",
    "dt = pd.read_stata('../input/decisions_data.dta')             # full sample\n",
    "ind_keep = pd.read_csv('../input/ind_to_keep.csv')   # import csv with ID of subjects to keep \n",
    "\n",
    "# drop subjects whose IDs are not listed in the data_drop dataframe (28 individuals)\n",
    "\n",
    "dt = dt[dt.wid.isin(ind_keep.wid_col1)] # this is the primary sample for the aggregate estimates (72 individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove observations when a bonus was offered and create dummy variables that will be useful for estimation. pb is equal to one if the subject completed 10 mandatory tasks on subject-day (this is used to estimate the projection bias parameter &alpha;). ind_effort10 and ind_effort110 are two dummy variables equal to one if the subject completed 10 or 110 tasks respectively and they are used for the Tobit correction when computing the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove observations when a bonus was offered and create dummy variables. \n",
    "\n",
    "dt = dt[dt.bonusoffered !=1]   # remove observations when a bonus was offered\n",
    "dt['pb']= dt['workdone1']/10   # pb dummy variable. workdone1 can either be 10 or 0, so dividing the variable by 10 creates our dummy\n",
    "dt['ind_effort10']  = (dt['effort']==10).astype(int)   # ind_effort10 dummy\n",
    "dt['ind_effort110'] = (dt['effort']==110).astype(int)  # ind_effort110 dummy\n",
    "dt.index = np.arange(len(dt.wid))                      # correct the index. The index should go from 0 to 8048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Model and the Likelihood (Section 3 in Paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent needs to choose the optimal effort e to solve a simple tradeoff problem between disutility of effort and consumption utility derived from the consequent payment. More specifically, the agent takes a decision at a time k to complete a certain number of tasks at time t and getting paid a wage w per task at time T. Assuming the agent discounts utility using quasi-hyperbolic present bias &beta; and has a convex cost function C(e) the problem can be conveniently written as:\n",
    "\n",
    "$$ \\max_{{e}} \\; \\delta^{T-k}⋅(e⋅w)- \\frac{1}{\\beta^{I(k=t)}}⋅\\frac{1}{\\beta_h^{I(p=1)}}⋅\\delta^{t-k}⋅ \\frac{e^\\gamma}{\\phi⋅\\gamma} $$\n",
    "\n",
    "Where the last term is a two parameter power cost function, I(k=t) is an indicator function equal to one if the decision occurs in the same period as the effort and I(p=1) is an indicator that the decision is a prediction. &beta;<sub>h</sub> is in fact the perceived present bias parameter, or how much aware is an agent of their present bias, while &delta; is the standard time discounting parameter. Taking the derivative of the maximization problem above with respect to effort yields the following first order condition:\n",
    "\n",
    "$$  e^*= \\left(\\frac{\\delta^{T-k}⋅\\phi⋅w}{\\frac{1}{\\beta^{I(k=t)}}⋅\\frac{1}{\\beta_h^{I(p=1)}}⋅\\delta^{t-k}} \\right)^{\\frac{1}{\\gamma-1}} $$\n",
    "\n",
    "This is the optimal effort level, or what we will call in the code the predicted choice. To model heterogeneity the authors assume that the observed effort is distributed as the predicted effort plus an implementation error which is Gaussian with mean zero and standard deviation sigma, so that the likelihood of observing an effort decision e<sub>j</sub> in the data is equal to:\n",
    "\n",
    "$$ L(e_j)= \\phi \\left(\\frac{e^*_j-e_j}{\\sigma}\\right)$$\n",
    "\n",
    "Where &phi; here represents the pdf of a standard normal. To deal with corner solutions we apply a Tobit correction, so that the likelihood to maximize is:\n",
    "\n",
    "$$ L^{tobit}(e_j)=(1-I(e=10)-I(e=110))⋅\\phi \\left(\\frac{e^*_j-e_j}{\\sigma}\\right) + I(e=10)⋅(1- \\Phi \\left(\\frac{e_j^*-10}{\\sigma}\\right))+I(e=110)⋅ \\Phi \\left(\\frac{e_j^*-110}{\\sigma}\\right) $$\n",
    "\n",
    "&Phi;(⋅) is the cdf of a standard normal, while I(e=10) and I(e=110) are the indicators ind_effort10 and ind_effort110 explained above. Note that to keep the code simple we call effort in this notebook the tasks chosen by an agent, which can range from a minimum of 0 to a maximum of 100, plus the compulsory 10 tasks. In the paper the authors call effort just the tasks chosen by the agent so they add 10 to get total effort. This explains the differences you may see with equation (7), (8) and (10) of section 3 in the paper. We will minimize the negative of the sum of the logarithms of L<sup>tobit</sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function negloglike computes the negative of the log likelihood of observing our data given the parameters of the model.\n",
    "\n",
    "# parameters:\n",
    "\n",
    "# beta is the present bias parameter\n",
    "# betahat is the perceived present bias parameter\n",
    "# delta is the usual time-discounting parameter\n",
    "# gamma and phi are the two parameters controlling the cost of effort function\n",
    "# alpha is the projection bias parameter\n",
    "# sigma is the standard deviation of the normal error term ϵ\n",
    "\n",
    "# args:\n",
    "\n",
    "# netdistance is (T-k)-(t-k) = T-t, the difference between the payment date T and the work time t\n",
    "# wage is the amount paid per task in a certain session\n",
    "# today is a dummy variable equal to one if the decision involves the choice of work today\n",
    "# prediction is a dummy variable equal to one if the decision involves the choice of work in the future\n",
    "# pb is a dummy equal to one if the subject completed 10 mandatory tasks on subject-day \n",
    "# effort is the number of tasks completed by a subject in a session. It can range from a minimum of 10 to a maximum of 110\n",
    "# ind_effort10 is a dummy equal to one if the subject's effort was equal to 10\n",
    "# ind_effort110 is a dummy equal to one if the subject's effort was equal to 110\n",
    "\n",
    "\n",
    "def negloglike(params, *args):\n",
    "    \n",
    "    beta, betahat, delta, gamma, phi, alpha, sigma = params\n",
    "    netdistance, wage, today, prediction, pb, effort, ind_effort10, ind_effort110 = args\n",
    "    \n",
    "    # We use np.array to allow for element-wise operations\n",
    "    \n",
    "    netdistance = np.array(netdistance)\n",
    "    wage = np.array(wage)\n",
    "    today = np.array(today)\n",
    "    prediction = np.array(prediction)\n",
    "    pb = np.array(pb)\n",
    "    effort = np.array(effort)\n",
    "    ind_effort10 = np.array(ind_effort10)\n",
    "    ind_effort110 = np.array(ind_effort110)\n",
    "    \n",
    "    # predchoice is the predicted choice coming from the optimality condition of the subject\n",
    "    \n",
    "    predchoice=((phi*(delta**netdistance)*(beta**today)*(betahat**prediction)*wage)**(1/(gamma-1)))-pb*alpha\n",
    "    \n",
    "    # prob is a 1x8049 vector containing the probability of observing the effort of an individual. If effort is 10 or 110 we apply a Tobit correction\n",
    "    \n",
    "    prob = (1-ind_effort10-ind_effort110)*norm.pdf(effort, predchoice, sigma)+ind_effort10*(1 - norm.cdf((predchoice-effort)/sigma))+ind_effort110*norm.cdf((predchoice-effort)/sigma)\n",
    "            \n",
    "    # we now look at the vector prob and add a small value close to zero if prob=0 or subtract a small value close to zero if prob=1. This is necessary to avoid problems when taking logs\n",
    "        \n",
    "    index_p0 = [i for i in range(0,len(prob)) if prob[i]==0] # vector containing the indexes when prob=0\n",
    "    index_p1 = [i for i in range(0,len(prob)) if prob[i]==1] # vector containing the indexes when prob=1\n",
    "    \n",
    "    # use a for loop to change the values\n",
    "    \n",
    "    for i in index_p0:\n",
    "        prob[i] = 1E-4\n",
    "        \n",
    "    for i in index_p1:\n",
    "        prob[i] = 1 - 1E-4\n",
    "    \n",
    "    negll = - np.sum(np.log(prob)) # negative log likelihood\n",
    "    \n",
    "    return negll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Estimation\n",
    "\n",
    "### Point estimates and standard errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now estimate the model. First we need to initialize a vector with the starting parameters for the minimization algorithm. We then minimize the negative log-likelihood function using the scipy.optimize package and the Nelder-Mead algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial guesses (same as the ones used by the authors in their do.file) and the arguments for the function to minimize\n",
    "\n",
    "# starting parameters for the algorithm\n",
    "\n",
    "beta_init, betahat_init, delta_init, gamma_init, phi_init, alpha_init, sigma_init = 0.8, 1, 1, 2, 500, 7, 40\n",
    "par_init = [beta_init, betahat_init, delta_init, gamma_init, phi_init, alpha_init, sigma_init]\n",
    "\n",
    "# args necessary for the function to minimize\n",
    "\n",
    "mle_args = (dt['netdistance'],dt['wage'],dt['today'],dt['prediction'],dt['pb'],dt['effort'],dt['ind_effort10'],dt['ind_effort110'])\n",
    "\n",
    "# we now find the estimates using the scipy.optimize package\n",
    "\n",
    "sol = opt.minimize(negloglike, par_init, args=(mle_args), method='Nelder-Mead', options={'maxiter': 1500})\n",
    "res = sol.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now estimate individual cluster robust standard errors. These are computed by taking the square root of the diagonal elements of the following matrix: \n",
    "\n",
    "$$ Adj⋅(H^{-1} @ G @ H^{-1}) $$ \n",
    "\n",
    "Where Adj is an adjustment for the degree of freedoms and the number of clusters:\n",
    "\n",
    "$$ Adj = \\frac{Nr.observations-1}{Nr.observations-Nr.parameters}⋅\\frac{Nr.clusters}{Nr.clusters-1} $$ \n",
    "\n",
    "H<sup>-1</sup> is the inverse of the hessian of the negative log-likelihood evaluated in the minimum (our estimates). @ stands for matrix multiplication and G is a 5x5 matrix of gradient contributions. Call the gradient of the log likelihood function for a generic individual i in the following way:\n",
    "\n",
    "$$  g_i(y|\\theta) = [log f_i(y|\\theta)]' = \\frac{\\partial}{\\partial \\theta} log f_i(y|\\theta) $$\n",
    "\n",
    "Where &theta; is the parameters vector and f<sub>i</sub>(y|&theta;) the likelihood function. Then G is defined as follows:\n",
    "\n",
    "$$ G = \\sum_j \\left[\\sum_{i \\in c_j}g_i(y|\\hat{\\theta})\\right]^T\\left[\\sum_{i \\in c_j}g_i(y|\\hat{\\theta})\\right] $$\n",
    "\n",
    "J is the number of clusters (in our case the number of unique individuals = 72) and c<sub>j</sub> is a generic cluster j, that includes all observations for a specific individual (in our case 130). For more information on how to compute standard errors when using maximum likelihood please refer for example to the following paper: David A. Freedman (2006) On The So-Called “Huber Sandwich Estimator” and “Robust Standard Errors”, *The American Statistician*, 60:4, 299-302."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an auxiliary function.\n",
    "# This function takes as input a dataset and a scalar and returns a dataframe containing only observations for individual whose ID is equal to that scalar\n",
    "\n",
    "def getind(dataset,wid):\n",
    "    dataset_ind = dataset[dataset.wid == wid]\n",
    "    return dataset_ind\n",
    "\n",
    "# Define the function that computes the matrix of individual gradient contribution G. We compute the gradient numerically using the numdifftools package\n",
    "\n",
    "def gradcontr(dataset):\n",
    "    G = 0.0\n",
    "    for wid in np.unique(dataset.wid): # loop over the individuals IDs\n",
    "        dt_ind = getind(dataset,wid)   # dataframe for individual wid\n",
    "        \n",
    "        # args needed to compute the individual likelihood\n",
    "        args_ind=[dt_ind['netdistance'], dt_ind['wage'], dt_ind['today'], dt_ind['prediction'], dt_ind['pb'], \n",
    "            dt_ind['effort'], dt_ind['ind_effort10'],dt_ind['ind_effort110']]\n",
    "        \n",
    "        gradcontr_ind = gradfun(res,*args_ind) # gradient of the negloglike function evaluated at the minimum\n",
    "        \n",
    "        G += np.outer(gradcontr_ind,gradcontr_ind) # G is the sum over individuals of the outer product of gradcontr_ind\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the individual cluster robust standard errors\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # This is to avoid showing RuntimeWarning regarding overflow when computing the hessian. This warning does not affect the results.\n",
    "                                  # We could avoid it by adding a check on the value of predchoice. We are using this command since we are sure of the results,\n",
    "                                  # we do not suggest suppressing errors otherwise.\n",
    "\n",
    "# Compute the hessian\n",
    "Hfun = ndt.Hessian(negloglike)\n",
    "hessian = Hfun(res,*mle_args)      # numerical hessian\n",
    "hess_inv = np.linalg.inv(hessian)  # inverse of the hessian\n",
    "\n",
    "# Compute the matrix of gradient contribution\n",
    "gradfun = ndt.Gradient(negloglike)\n",
    "grad_contribution = gradcontr(dt)\n",
    "\n",
    "# Compute the adjustment for degree of freedoms and number of clusters\n",
    "adj = (len(dt.wid)-1)/(len(dt.wid)-len(res)) * len(np.unique(dt.wid))/(len(np.unique(dt.wid))-1)\n",
    "\n",
    "varcov_estimates = adj *(hess_inv @ grad_contribution @ hess_inv) # var-cov matrix of our estimates\n",
    "se_cluster = np.sqrt(np.diag((varcov_estimates))) # individual cluster robust standard errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing\n",
    "\n",
    "We now do some hypothesis testing on the parameters we obtained. We compute the z-test statistics and the corresponding p-values to check if beta, betahat or delta are statistically different from one. We then compute the p-value of a z-test to check if the parameter for projection bias is statistically different from zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the z-test statistics and the corresponding p-values to check if beta, betahat, delta are statistically different from one\n",
    "\n",
    "zvalues_1 = (np.array(res[0:3])-1)/np.array(se_cluster[0:3]) # the first three elements are for beta (position 0), betahat (position 1) and delta (position 2)\n",
    "pvalues_1 = 2*(1-norm.cdf(np.abs(zvalues_1),0,1))\n",
    "\n",
    "# Now compute the z-test statistics and the corresponding p-value for H0: alpha different from 0\n",
    "\n",
    "zvalue_a = (np.array(res[5]))/np.array(se_cluster[5]) \n",
    "pvalue_a = 2*(1-norm.cdf(np.abs(zvalue_a),0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Print and Save Estimation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a table with point estimates and individual cluster robust standard errors. We then save the results as a csv file in the output folder and print the results. This replicates column 1 of Table 1 in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the results and save it as a csv file in output. We round the results up to the 3rd decimal.\n",
    "\n",
    "parameters_name = [\"Present Bias β\",\n",
    "                   \"Naive Pres. Bias β_h\",\n",
    "                   \"Discount Factor δ\",\n",
    "                   \"Cost Curvature γ\",\n",
    "                   \"Cost Slope ϕ\",\n",
    "                   \"Proj Task Reduction α\",\n",
    "                   \"Sd of error term σ\"]\n",
    "\n",
    "Table_1 = pd.DataFrame({'parameters':parameters_name,'estimates':np.round(res,3),'standarderr':np.round(se_cluster,3)})\n",
    "\n",
    "Table_1.to_csv('../output/table1_python.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1: Primary aggregate structural estimation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameters</th>\n",
       "      <th>estimates</th>\n",
       "      <th>standarderr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Present Bias β</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Pres. Bias β_h</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discount Factor δ</td>\n",
       "      <td>1.003</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cost Curvature γ</td>\n",
       "      <td>2.145</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cost Slope ϕ</td>\n",
       "      <td>723.974</td>\n",
       "      <td>249.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Proj Task Reduction α</td>\n",
       "      <td>7.307</td>\n",
       "      <td>2.702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sd of error term σ</td>\n",
       "      <td>42.625</td>\n",
       "      <td>3.314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              parameters  estimates  standarderr\n",
       "0         Present Bias β      0.835        0.040\n",
       "1   Naive Pres. Bias β_h      0.999        0.011\n",
       "2      Discount Factor δ      1.003        0.003\n",
       "3       Cost Curvature γ      2.145        0.076\n",
       "4           Cost Slope ϕ    723.974      249.042\n",
       "5  Proj Task Reduction α      7.307        2.702\n",
       "6     Sd of error term σ     42.625        3.314"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations: 8,049\n",
      "Number of participants: 72\n",
      "Log Likelihood: -28,412\n",
      "H_0(β=1) 0.00\n",
      "H_0(β_h=1): 0.92\n",
      "H_0(α=0): 0.007\n",
      "H_0(δ=1): 0.38\n"
     ]
    }
   ],
   "source": [
    "# Print the results. Small differences in the standard errors are probably due to the numerically computed hessian/gradient.\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"Table 1: Primary aggregate structural estimation\")\n",
    "display(Table_1)\n",
    "print(\"Number of observations:\",\"{:,}\".format(len(dt.wid)))\n",
    "print(\"Number of participants:\",\"{:,}\".format(len(np.unique(dt.wid))))\n",
    "print(\"Log Likelihood:\",\"{:,.0f}\".format(-sol.fun))\n",
    "print(\"H_0(β=1)\",\"{:,.2f}\" .format(np.round(pvalues_1[0],3)))\n",
    "print(\"H_0(β_h=1):\",\"{:,.2f}\".format(np.round(pvalues_1[1],2)))\n",
    "print(\"H_0(α=0):\",\"{:,.3f}\".format(np.round(pvalue_a,3)))\n",
    "print(\"H_0(δ=1):\",\"{:,.2f}\".format(np.round(pvalues_1[2],2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
